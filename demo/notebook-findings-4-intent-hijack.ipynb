{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Variable Definition for different findings\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"user_name = \\\"jingpeiwu1786\\\"\\n\",\n",
    "    \"dataset_name = \\\"break-oss-finding-4\\\"\\n\",\n",
    "    \"file_path = \\\"breakoss-findings-4-intent-hijack.json\\\"\\n\",\n",
    "    \"method = ''\\n\",\n",
    "    \"# method = 'cot_bypass'\\n\",\n",
    "    \"# method = 'gcg'\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Environment Setup\\n\",\n",
    "    \"\\n\",\n",
    "    \"- following https://www.kaggle.com/code/bwandowando/i-m-sorry-but-i-cant-help-with-that\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"execution\": {\n",
    "     \"iopub.execute_input\": \"2025-08-26T16:12:42.099250Z\",\n",
    "     \"iopub.status.busy\": \"2025-08-26T16:12:42.099042Z\",\n",
    "     \"iopub.status.idle\": \"2025-08-26T16:13:12.726890Z\",\n",
    "     \"shell.execute_reply\": \"2025-08-26T16:13:12.725943Z\",\n",
    "     \"shell.execute_reply.started\": \"2025-08-26T16:12:42.099233Z\"\n",
    "    },\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"!pip uninstall numpy scikit-learn scipy cupy-cuda12x torch torchaudio torchvision transformers triton kernels --yes\\n\",\n",
    "    \"!pip cache purge\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"execution\": {\n",
    "     \"iopub.execute_input\": \"2025-08-26T16:13:12.730852Z\",\n",
    "     \"iopub.status.busy\": \"2025-08-26T16:13:12.730666Z\",\n",
    "     \"iopub.status.idle\": \"2025-08-26T16:16:58.549999Z\",\n",
    "     \"shell.execute_reply\": \"2025-08-26T16:16:58.549087Z\",\n",
    "     \"shell.execute_reply.started\": \"2025-08-26T16:13:12.730829Z\"\n",
    "    },\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"!pip install numpy==1.26.4 scikit-learn==1.5.2 scipy==1.15.3 cupy-cuda12x==13.5.1\\n\",\n",
    "    \"!pip install torch==2.8.0 --index-url https://download.pytorch.org/whl/test/cu128\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"execution\": {\n",
    "     \"iopub.execute_input\": \"2025-08-26T16:16:58.551960Z\",\n",
    "     \"iopub.status.busy\": \"2025-08-26T16:16:58.551719Z\",\n",
    "     \"iopub.status.idle\": \"2025-08-26T16:17:09.173563Z\",\n",
    "     \"shell.execute_reply\": \"2025-08-26T16:17:09.172676Z\",\n",
    "     \"shell.execute_reply.started\": \"2025-08-26T16:16:58.551936Z\"\n",
    "    },\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# !git clone https://github.com/huggingface/transformers.git\\n\",\n",
    "    \"# !pip install transformers/.[torch]\\n\",\n",
    "    \"!pip install transformers==4.55.4\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"execution\": {\n",
    "     \"iopub.execute_input\": \"2025-08-26T16:17:09.175018Z\",\n",
    "     \"iopub.status.busy\": \"2025-08-26T16:17:09.174769Z\",\n",
    "     \"iopub.status.idle\": \"2025-08-26T16:17:24.523503Z\",\n",
    "     \"shell.execute_reply\": \"2025-08-26T16:17:24.522578Z\",\n",
    "     \"shell.execute_reply.started\": \"2025-08-26T16:17:09.174990Z\"\n",
    "    },\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"!pip install git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\\n\",\n",
    "    \"!pip install kernels\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"execution\": {\n",
    "     \"iopub.execute_input\": \"2025-08-26T16:17:24.524936Z\",\n",
    "     \"iopub.status.busy\": \"2025-08-26T16:17:24.524653Z\",\n",
    "     \"iopub.status.idle\": \"2025-08-26T16:17:24.531444Z\",\n",
    "     \"shell.execute_reply\": \"2025-08-26T16:17:24.530675Z\",\n",
    "     \"shell.execute_reply.started\": \"2025-08-26T16:17:24.524904Z\"\n",
    "    },\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from IPython.core.display import HTML\\n\",\n",
    "    \"HTML(\\\"<script>Jupyter.notebook.kernel.restart()</script>\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Json Loading\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"### option 1: first click \\\"Add input\\\" under File tag, choose the corresponding dataset / json file\\n\",\n",
    "    \"# import json\\n\",\n",
    "    \"\\n\",\n",
    "    \"# data_path = f\\\"/kaggle/input/{dataset_name}/{file_path}\\\"\\n\",\n",
    "    \"# with open(data_path, \\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n\",\n",
    "    \"#     hf_dataset = json.load(f)\\n\",\n",
    "    \"# print(hf_dataset)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"### option 2: import dataset \\n\",\n",
    "    \"# Install dependencies as needed:\\n\",\n",
    "    \"# !pip install kagglehub[hf-datasets]\\n\",\n",
    "    \"import kagglehub\\n\",\n",
    "    \"from kagglehub import KaggleDatasetAdapter\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set the path to the file you'd like to load\\n\",\n",
    "    \"# file_path = \\\"example-harmony-findings.json\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the latest version\\n\",\n",
    "    \"hf_dataset = kagglehub.load_dataset(\\n\",\n",
    "    \"  KaggleDatasetAdapter.HUGGING_FACE,\\n\",\n",
    "    \"  f\\\"{user_name}/{dataset_name}\\\",\\n\",\n",
    "    \"  # f\\\"{user_name}/{dataset_name}/version/2\\\",\\n\",\n",
    "    \"  file_path,\\n\",\n",
    "    \"  # Provide any additional arguments like \\n\",\n",
    "    \"  # sql_query, hf_kwargs, or pandas_kwargs. See \\n\",\n",
    "    \"  # the documenation for more information:\\n\",\n",
    "    \"  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterhugging_face\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Hugging Face Dataset:\\\", hf_dataset)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"hf_dataset = hf_dataset[0]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Inference\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"execution\": {\n",
    "     \"iopub.execute_input\": \"2025-08-26T16:17:31.326716Z\",\n",
    "     \"iopub.status.busy\": \"2025-08-26T16:17:31.326452Z\",\n",
    "     \"iopub.status.idle\": \"2025-08-26T16:17:38.395783Z\",\n",
    "     \"shell.execute_reply\": \"2025-08-26T16:17:38.395184Z\",\n",
    "     \"shell.execute_reply.started\": \"2025-08-26T16:17:31.326694Z\"\n",
    "    },\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from transformers import AutoModelForCausalLM, AutoTokenizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import random\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"def set_seed(seed=42):\\n\",\n",
    "    \"    random.seed(seed)\\n\",\n",
    "    \"    np.random.seed(seed)\\n\",\n",
    "    \"    torch.manual_seed(seed)\\n\",\n",
    "    \"    torch.cuda.manual_seed(seed)\\n\",\n",
    "    \"    torch.cuda.manual_seed_all(seed)\\n\",\n",
    "    \"    os.environ[\\\"CUBLAS_WORKSPACE_CONFIG\\\"] = \\\":4096:8\\\"\\n\",\n",
    "    \"    torch.use_deterministic_algorithms(True)\\n\",\n",
    "    \"    torch.backends.cudnn.deterministic = True\\n\",\n",
    "    \"    torch.backends.cudnn.benchmark = False\\n\",\n",
    "    \"\\n\",\n",
    "    \"set_seed()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"execution\": {\n",
    "     \"iopub.execute_input\": \"2025-08-26T16:17:39.360422Z\",\n",
    "     \"iopub.status.busy\": \"2025-08-26T16:17:39.359924Z\",\n",
    "     \"iopub.status.idle\": \"2025-08-26T16:20:31.936049Z\",\n",
    "     \"shell.execute_reply\": \"2025-08-26T16:20:31.935444Z\",\n",
    "     \"shell.execute_reply.started\": \"2025-08-26T16:17:39.360390Z\"\n",
    "    },\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"model_name = \\\"openai/gpt-oss-20b\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"tokenizer = AutoTokenizer.from_pretrained(model_name)\\n\",\n",
    "    \"model = AutoModelForCausalLM.from_pretrained(\\n\",\n",
    "    \"    model_name, \\n\",\n",
    "    \"    torch_dtype=\\\"auto\\\", \\n\",\n",
    "    \"    device_map=\\\"auto\\\", \\n\",\n",
    "    \"    low_cpu_mem_usage=True,\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"execution\": {\n",
    "     \"iopub.execute_input\": \"2025-08-26T16:44:06.209061Z\",\n",
    "     \"iopub.status.busy\": \"2025-08-26T16:44:06.208293Z\",\n",
    "     \"iopub.status.idle\": \"2025-08-26T16:44:06.215468Z\",\n",
    "     \"shell.execute_reply\": \"2025-08-26T16:44:06.214549Z\",\n",
    "     \"shell.execute_reply.started\": \"2025-08-26T16:44:06.209038Z\"\n",
    "    },\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def generate_response(model, prompt):\\n\",\n",
    "    \"    messages = [\\n\",\n",
    "    \"        # {\\n\",\n",
    "    \"        #     \\\"role\\\": \\\"system\\\",\\n\",\n",
    "    \"        #     \\\"content\\\": \\\"Reasoning: high\\\",\\n\",\n",
    "    \"        #     # \\\"content\\\": developer_prompt\\n\",\n",
    "    \"        # },\\n\",\n",
    "    \"        {\\n\",\n",
    "    \"            \\\"role\\\": \\\"user\\\",\\n\",\n",
    "    \"            # \\\"content\\\": prompt,\\n\",\n",
    "    \"            \\\"content\\\": prompt,\\n\",\n",
    "    \"            # \\\"content\\\": cot_bypass.format(harmful_prompts=prompt) if method == \\\"cot_bypass\\\" else prompt\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    inputs = tokenizer.apply_chat_template(\\n\",\n",
    "    \"        messages,\\n\",\n",
    "    \"        add_generate_prompt=True,\\n\",\n",
    "    \"        return_tensors=\\\"pt\\\",\\n\",\n",
    "    \"        # return_dict=True,\\n\",\n",
    "    \"        tokenize=False,\\n\",\n",
    "    \"    )#.to(model.device)\\n\",\n",
    "    \"    # print(inputs)\\n\",\n",
    "    \"    inputs = inputs.replace(\\\"Reasoning: medium\\\", \\\"Reasoning: high\\\")\\n\",\n",
    "    \"    inputs = tokenizer(inputs, return_tensors=\\\"pt\\\").to(model.device)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    inference_config = {\\n\",\n",
    "    \"        \\\"max_new_tokens\\\": 100000,\\n\",\n",
    "    \"        \\\"temperature\\\": 0.7,\\n\",\n",
    "    \"        \\\"top_p\\\": 0.9,\\n\",\n",
    "    \"        \\\"repetition_penalty\\\": 1.0,\\n\",\n",
    "    \"        # \\\"num_return_sequences\\\": 3\\n\",\n",
    "    \"        # \\\"reasoning_level\\\": 'high',    # not used by the model \\n\",\n",
    "    \"        # \\\"seed\\\": 42    # not used by the model\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    generated = model.generate(**inputs, **inference_config)\\n\",\n",
    "    \"    for i, output_ids in enumerate(generated):\\n\",\n",
    "    \"        resp = tokenizer.decode(generated[i][inputs[\\\"input_ids\\\"].shape[-1]:], skip_special_tokens=False)\\n\",\n",
    "    \"        print(resp, '\\\\n')\\n\",\n",
    "    \"    # response = tokenizer.decode(generated[0][inputs[\\\"input_ids\\\"].shape[-1]:], skip_special_tokens=False)\\n\",\n",
    "    \"    # print(response)\\n\",\n",
    "    \"    return generated\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"developer_pattern = r'<\\\\|start\\\\|>developer<\\\\|message\\\\|># Instructions(.*?)<\\\\|end\\\\|>'\\n\",\n",
    "    \"if method == 'cot_bypass':\\n\",\n",
    "    \"    user_pattern = r'<\\\\|start\\\\|>user<\\\\|message\\\\|>(.*)<\\\\|end\\\\|><\\\\|start\\\\|>assistant' # greedy matching until the tags added by the tokenizer.apply_chat_template\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    user_pattern = r'<\\\\|start\\\\|>user<\\\\|message\\\\|>(.*?)<\\\\|end\\\\|>'\\n\",\n",
    "    \"\\n\",\n",
    "    \"# developer_match = re.search(developer_pattern, hf_dataset[0][\\\"harmony_response_walkthroughs\\\"][0], re.DOTALL)\\n\",\n",
    "    \"# user_match = re.search(user_pattern, hf_dataset[0][\\\"harmony_response_walkthroughs\\\"][0], re.DOTALL)\\n\",\n",
    "    \"developer_match = re.search(developer_pattern, hf_dataset[\\\"harmony_response_walkthroughs\\\"][0], re.DOTALL)\\n\",\n",
    "    \"user_match = re.search(user_pattern, hf_dataset[\\\"harmony_response_walkthroughs\\\"][0], re.DOTALL)\\n\",\n",
    "    \"developer_prompt, user_prompt = \\\"\\\", \\\"\\\"\\n\",\n",
    "    \"if developer_match:\\n\",\n",
    "    \"    developer_prompt = developer_match.group(1).strip()\\n\",\n",
    "    \"if user_match:\\n\",\n",
    "    \"    user_prompt = user_match.group(1).strip()\\n\",\n",
    "    \"print(developer_prompt)\\n\",\n",
    "    \"print(user_prompt)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"**NOTE**: please rerun the following cell multiple times (should be < 10) if the first response does not give described behavior.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"response = generate_response(model, user_prompt)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"execution\": {\n",
    "     \"iopub.execute_input\": \"2025-08-26T17:28:31.874139Z\",\n",
    "     \"iopub.status.busy\": \"2025-08-26T17:28:31.873542Z\",\n",
    "     \"iopub.status.idle\": \"2025-08-26T17:28:31.879657Z\",\n",
    "     \"shell.execute_reply\": \"2025-08-26T17:28:31.878851Z\",\n",
    "     \"shell.execute_reply.started\": \"2025-08-26T17:28:31.874118Z\"\n",
    "    },\n",
    "    \"trusted\": true\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(json.dumps(tokenizer.decode(response[0])))\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kaggle\": {\n",
    "   \"accelerator\": \"nvidiaTeslaT4\",\n",
    "   \"dataSources\": [\n",
    "    {\n",
    "     \"databundleVersionId\": 13289915,\n",
    "     \"sourceId\": 106949,\n",
    "     \"sourceType\": \"competition\"\n",
    "    },\n",
    "    {\n",
    "     \"datasetId\": 8127106,\n",
    "     \"sourceId\": 12849654,\n",
    "     \"sourceType\": \"datasetVersion\"\n",
    "    },\n",
    "    {\n",
    "     \"datasetId\": 8132624,\n",
    "     \"sourceId\": 12857749,\n",
    "     \"sourceType\": \"datasetVersion\"\n",
    "    },\n",
    "    {\n",
    "     \"datasetId\": 8135371,\n",
    "     \"sourceId\": 12861758,\n",
    "     \"sourceType\": \"datasetVersion\"\n",
    "    },\n",
    "    {\n",
    "     \"datasetId\": 8135380,\n",
    "     \"sourceId\": 12861770,\n",
    "     \"sourceType\": \"datasetVersion\"\n",
    "    }\n",
    "   ],\n",
    "   \"dockerImageVersionId\": 31089,\n",
    "   \"isGpuEnabled\": true,\n",
    "   \"isInternetEnabled\": true,\n",
    "   \"language\": \"python\",\n",
    "   \"sourceType\": \"notebook\"\n",
    "  },\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.11.13\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ],
   "id": "9fc55fb9510eff46"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

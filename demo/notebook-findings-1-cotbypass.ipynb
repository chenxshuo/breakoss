{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Definition for different findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_name = \"chenxshuo\"\n",
    "dataset_name = \"breakoss-findings-1-cot-bypass\"\n",
    "file_path = \"breakoss-findings-1-cotbypass.json\"\n",
    "method = 'cot_bypass'\n",
    "# method = 'gcg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "\n",
    "- following https://www.kaggle.com/code/bwandowando/i-m-sorry-but-i-cant-help-with-that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T16:12:42.099250Z",
     "iopub.status.busy": "2025-08-26T16:12:42.099042Z",
     "iopub.status.idle": "2025-08-26T16:13:12.726890Z",
     "shell.execute_reply": "2025-08-26T16:13:12.725943Z",
     "shell.execute_reply.started": "2025-08-26T16:12:42.099233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall numpy scikit-learn scipy cupy-cuda12x torch torchaudio torchvision transformers triton kernels --yes\n",
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T16:13:12.730852Z",
     "iopub.status.busy": "2025-08-26T16:13:12.730666Z",
     "iopub.status.idle": "2025-08-26T16:16:58.549999Z",
     "shell.execute_reply": "2025-08-26T16:16:58.549087Z",
     "shell.execute_reply.started": "2025-08-26T16:13:12.730829Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 scikit-learn==1.5.2 scipy==1.15.3 cupy-cuda12x==13.5.1\n",
    "!pip install torch==2.8.0 --index-url https://download.pytorch.org/whl/test/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T16:16:58.551960Z",
     "iopub.status.busy": "2025-08-26T16:16:58.551719Z",
     "iopub.status.idle": "2025-08-26T16:17:09.173563Z",
     "shell.execute_reply": "2025-08-26T16:17:09.172676Z",
     "shell.execute_reply.started": "2025-08-26T16:16:58.551936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/huggingface/transformers.git\n",
    "# !pip install transformers/.[torch]\n",
    "!pip install transformers==4.55.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T16:17:09.175018Z",
     "iopub.status.busy": "2025-08-26T16:17:09.174769Z",
     "iopub.status.idle": "2025-08-26T16:17:24.523503Z",
     "shell.execute_reply": "2025-08-26T16:17:24.522578Z",
     "shell.execute_reply.started": "2025-08-26T16:17:09.174990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels\n",
    "!pip install kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T16:17:24.524936Z",
     "iopub.status.busy": "2025-08-26T16:17:24.524653Z",
     "iopub.status.idle": "2025-08-26T16:17:24.531444Z",
     "shell.execute_reply": "2025-08-26T16:17:24.530675Z",
     "shell.execute_reply.started": "2025-08-26T16:17:24.524904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### option 1: first click \"Add input\" under File tag, choose the corresponding dataset / json file\n",
    "# import json\n",
    "\n",
    "# data_path = f\"/kaggle/input/{dataset_name}/{file_path}\"\n",
    "# with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     hf_dataset = json.load(f)\n",
    "# print(hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### option 2: import dataset \n",
    "# Install dependencies as needed:\n",
    "# !pip install kagglehub[hf-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "# file_path = \"example-harmony-findings.json\"\n",
    "\n",
    "# Load the latest version\n",
    "hf_dataset = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.HUGGING_FACE,\n",
    "  f\"{user_name}/{dataset_name}/{file_path}\",\n",
    "  # f\"{user_name}/{dataset_name}/version/2\",\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Hugging Face Dataset:\", hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = hf_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T16:17:31.326716Z",
     "iopub.status.busy": "2025-08-26T16:17:31.326452Z",
     "iopub.status.idle": "2025-08-26T16:17:38.395783Z",
     "shell.execute_reply": "2025-08-26T16:17:38.395184Z",
     "shell.execute_reply.started": "2025-08-26T16:17:31.326694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T16:17:39.360422Z",
     "iopub.status.busy": "2025-08-26T16:17:39.359924Z",
     "iopub.status.idle": "2025-08-26T16:20:31.936049Z",
     "shell.execute_reply": "2025-08-26T16:20:31.935444Z",
     "shell.execute_reply.started": "2025-08-26T16:17:39.360390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"openai/gpt-oss-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=\"auto\", \n",
    "    device_map=\"auto\", \n",
    "    low_cpu_mem_usage=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T16:44:06.209061Z",
     "iopub.status.busy": "2025-08-26T16:44:06.208293Z",
     "iopub.status.idle": "2025-08-26T16:44:06.215468Z",
     "shell.execute_reply": "2025-08-26T16:44:06.214549Z",
     "shell.execute_reply.started": "2025-08-26T16:44:06.209038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_response(model, prompt):\n",
    "    messages = [\n",
    "        # {\n",
    "        #     \"role\": \"system\",\n",
    "        #     \"content\": \"Reasoning: high\",\n",
    "        #     # \"content\": developer_prompt\n",
    "        # },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            # \"content\": prompt,\n",
    "            \"content\": prompt,\n",
    "            # \"content\": cot_bypass.format(harmful_prompts=prompt) if method == \"cot_bypass\" else prompt\n",
    "        }\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generate_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        # return_dict=True,\n",
    "        tokenize=False,\n",
    "    )#.to(model.device)\n",
    "    # print(inputs)\n",
    "    inputs = inputs.replace(\"Reasoning: medium\", \"Reasoning: high\")\n",
    "    inputs = tokenizer(inputs, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    inference_config = {\n",
    "        \"max_new_tokens\": 100000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.0,\n",
    "        # \"num_return_sequences\": 3\n",
    "        # \"reasoning_level\": 'high',    # not used by the model \n",
    "        # \"seed\": 42    # not used by the model\n",
    "    }\n",
    "    \n",
    "    generated = model.generate(**inputs, **inference_config)\n",
    "    for i, output_ids in enumerate(generated):\n",
    "        resp = tokenizer.decode(generated[i][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=False)\n",
    "        print(resp, '\\n')\n",
    "    # response = tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=False)\n",
    "    # print(response)\n",
    "    return generated\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "developer_pattern = r'<\\|start\\|>developer<\\|message\\|># Instructions(.*?)<\\|end\\|>'\n",
    "if method == 'cot_bypass':\n",
    "    user_pattern = r'<\\|start\\|>user<\\|message\\|>(.*)<\\|end\\|><\\|start\\|>assistant' # greedy matching until the tags added by the tokenizer.apply_chat_template\n",
    "else:\n",
    "    user_pattern = r'<\\|start\\|>user<\\|message\\|>(.*?)<\\|end\\|>'\n",
    "\n",
    "# developer_match = re.search(developer_pattern, hf_dataset[0][\"harmony_response_walkthroughs\"][0], re.DOTALL)\n",
    "# user_match = re.search(user_pattern, hf_dataset[0][\"harmony_response_walkthroughs\"][0], re.DOTALL)\n",
    "developer_match = re.search(developer_pattern, hf_dataset[\"harmony_response_walkthroughs\"][0], re.DOTALL)\n",
    "user_match = re.search(user_pattern, hf_dataset[\"harmony_response_walkthroughs\"][0], re.DOTALL)\n",
    "developer_prompt, user_prompt = \"\", \"\"\n",
    "if developer_match:\n",
    "    developer_prompt = developer_match.group(1).strip()\n",
    "if user_match:\n",
    "    user_prompt = user_match.group(1).strip()\n",
    "print(developer_prompt)\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: please rerun the following cell multiple times (should be < 10) if the first response does not give described behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_response(model, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T17:28:31.874139Z",
     "iopub.status.busy": "2025-08-26T17:28:31.873542Z",
     "iopub.status.idle": "2025-08-26T17:28:31.879657Z",
     "shell.execute_reply": "2025-08-26T17:28:31.878851Z",
     "shell.execute_reply.started": "2025-08-26T17:28:31.874118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(json.dumps(tokenizer.decode(response[0])))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13289915,
     "sourceId": 106949,
     "sourceType": "competition"
    },
    {
     "datasetId": 8127106,
     "sourceId": 12849654,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8132624,
     "sourceId": 12857749,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8135371,
     "sourceId": 12861758,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8135380,
     "sourceId": 12861770,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
